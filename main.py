# -*- coding: utf-8 -*-
"""Sydney and Frankie - Difficulty Protocol Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12Fdy8D3-9nCQ1wUsuMv5euOROtB7VbFW

# Import statements
"""
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import nltk
import spacy
nltk.download('punkt')
nltk.download('stopwords')
import string
import re
from pandas import DataFrame, Series
from nltk.util import ngrams
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from gensim.models.tfidfmodel import TfidfModel
from gensim.corpora.dictionary import Dictionary
from wordcloud import WordCloud
!python -m spacy download en_core_web_md #must restart runtime after running this line
"""


"""# Import csv files as dataframes
To be able to import csv files, make sure to upload files first via the tabs on the left
"""
stud = pd.read_csv('Difficulty Protocol Data - Student Protocol.csv',encoding="utf-8",index_col=0)
profta = pd.read_csv('Difficulty Protocol Data - Faculty_TA Protocol.csv',encoding="utf-8",index_col=0)


"""# Stop Word Categories

1.   Getting rid of any descriptor words (i.e. briefly, basically, current, towards)
2.   Getting rid of typical classroom activities (i.e. talked, presented, covering)
3. Getting rid of words that donâ€™t give any insight (i.e. barely, spent, able, end, began)


"""# Create student bags of words"""
#Topics Covered
stud_topics_tkn = tokenize(stud,'topics_covered')
stud_topics_bag = bag_of_words(stud_topics_tkn)
print(stud_topics_bag)

stud_topics_ngrams = []
ngram_list = extract_ngrams(stud_topics_tkn,2)
for ngram in ngram_list:
    stud_topics_ngrams.append(ngram)
print("Student Topics:",stud_topics_ngrams,'\n')

#Concepts Struggled
stud_struggles_tkn = tokenize(stud,'struggle_concepts')
stud_struggles_bag = bag_of_words(stud_struggles_tkn)

stud_struggles_ngrams = []
ngram_list = extract_ngrams(stud_struggles_tkn,2)
for ngram in ngram_list:
    stud_struggles_ngrams.append(ngram)
print("Student Struggles:",stud_struggles_ngrams,'\n')

#Questions Asked
stud_questions_tkn = tokenize(stud,'questions_raised')
stud_questions_bag = bag_of_words(stud_questions_tkn)

stud_questions_ngrams = []
ngram_list = extract_ngrams(stud_questions_tkn,2)
for ngram in ngram_list:
    stud_questions_ngrams.append(ngram)
print("Student Questions:",stud_questions_ngrams,'\n')

#Surprise Questions
stud_surprise_tkn = tokenize(stud,'surprise_questions')
stud_surprise_bag = bag_of_words(stud_surprise_tkn)

stud_surprise_ngrams = []
ngram_list = extract_ngrams(stud_surprise_tkn,2)
for ngram in ngram_list:
    stud_surprise_ngrams.append(ngram)
print("Student Surprise Questions:",stud_surprise_ngrams,'\n')


"""# Create Prof./TA Bag of Words"""
#Topics Covered
profta_topics_tkn = tokenize(profta,'topics_covered')
profta_topics_bag = bag_of_words(profta_topics_tkn)

profta_topics_ngrams = []
ngram_list = extract_ngrams(profta_topics_tkn,2)
for ngram in ngram_list:
    profta_topics_ngrams.append(ngram)
print("Professor/TA Topics:",profta_topics_ngrams,'\n')

#Concepts Struggled
profta_struggles_tkn = tokenize(profta,'struggle_concepts')
profta_struggles_bag = bag_of_words(profta_struggles_tkn)

profta_struggles_ngrams = []
ngram_list = extract_ngrams(profta_struggles_tkn,2)
for ngram in ngram_list:
    profta_struggles_ngrams.append(ngram)
print("Professor/TA Struggles:",profta_struggles_ngrams,'\n')

#Questions Asked
profta_questions_tkn = tokenize(profta,'questions_raised')
profta_questions_bag = bag_of_words(profta_questions_tkn)

profta_questions_ngrams = []
ngram_list = extract_ngrams(profta_questions_tkn,2)
for ngram in ngram_list:
    profta_questions_ngrams.append(ngram)
print("Professor/TA Questions:",profta_questions_ngrams,'\n')

#Surprise Questions
profta_surprise_tkn = tokenize(profta,'surprise_questions')
profta_surprise_bag = bag_of_words(profta_surprise_tkn)

profta_surprise_ngrams = []
ngram_list = extract_ngrams(profta_surprise_tkn,2)
for ngram in ngram_list:
    profta_surprise_ngrams.append(ngram)
print("Professor/TA Surprise Questions:",profta_surprise_ngrams,'\n')


"""# Prints the Top 10 for Prof/TA & Students"""
#Topics Covered
print("Professor/TA Topics:")
top_profta_topics = top10Idf(profta_topics_bag)
print("Student Topics:")
top_stud_topics = top10Idf(stud_topics_bag)

#Struggles Encountered
print("Professor/TA Struggles:")
top_profta_struggles = top10Idf(profta_struggles_bag)
print("Student Struggles:")
top_stud_struggles = top10Idf(stud_struggles_bag)

#Questions Asked
print("Professor/TA Questions:")
top_profta_questions = top10Idf(profta_questions_bag)
print("Student Questions:")
top_stud_questions = top10Idf(stud_questions_bag)

#Surprise Questions Asked
print("Professor/TA Surprise Questions:")
top_profta_surprise = top10Idf(profta_surprise_bag)
print("Student Surprise Questions:")
top_stud_surprise = top10Idf(stud_surprise_bag)


"""# Prints the top 10 N-Grams"""
#Topics Covered
print("Professor/TA Topics:")
top_profta_topics2 = top10Idf(profta_topics_ngrams)
print("Student Topics:")
top_stud_topics2 = top10Idf(stud_topics_ngrams)

#Struggles Encountered
print("Professor/TA Struggles:")
top_profta_struggles2 = top10Idf(profta_struggles_ngrams)
print("Student Struggles:")
top_stud_struggles2 = top10Idf(stud_struggles_ngrams)

#Questions Asked
print("Professor/TA Questions:")
top_profta_questions2 = top10Idf(profta_questions_ngrams)
print("Student Questions:")
top_stud_questions2 = top10Idf(stud_questions_ngrams)

#Surprise Questions Asked
print("Professor/TA Surprise Questions:")
top_profta_surprise2 = top10Idf(profta_surprise_ngrams)
print("Student Surprise Questions:")
top_stud_surprise2 = top10Idf(stud_surprise_ngrams)


"""# Plotting Word Clouds"""
#Topics Covered
wc1 = wordcloud(top_profta_topics)
wc2 = wordcloud(top_profta_topics2)
wc3 = wordcloud(top_stud_topics)
wc4 = wordcloud(top_stud_topics2)
plotclouds(wc1,wc2,wc3,wc4,'cloud_topics')

#Questions Asked
wc1 = wordcloud(top_profta_questions)
wc2 = wordcloud(top_profta_questions2)
wc3 = wordcloud(top_stud_questions)
wc4 = wordcloud(top_stud_questions2)
plotclouds(wc1,wc2,wc3,wc4,'cloud_questions')

#Struggles Encountered
wc1 = wordcloud(top_profta_struggles)
wc2 = wordcloud(top_profta_struggles2)
wc3 = wordcloud(top_stud_struggles)
wc4 = wordcloud(top_stud_struggles2)
plotclouds(wc1,wc2,wc3,wc4,'cloud_struggles')

#Surprise Questions Asked
wc1 = wordcloud(top_profta_surprise)
wc2 = wordcloud(top_profta_surprise2)
wc3 = wordcloud(top_stud_surprise)
wc4 = wordcloud(top_stud_surprise2)
plotclouds(wc1,wc2,wc3,wc4,'cloud_surprise')


"""# Sentence TFIDF"""

def senttfidf(bag):
  model, corpus,dictionary = tfidfModelling(bag)
  big_list = []
  for doc in corpus: #Loops through each entry in the column
    weights = []
    for combo in model[doc]: #Loops through each Bigram-weight combination for each response
      weights.append(combo)
    weights = sorted(weights, key=lambda w : w[1], reverse=True) #Sorts the weights from highest to lowest for each response
    if len(weights) != 0: #makes sure that response is not blank
      big_list.append(weights[0]) #Appends only the highest weighted combination from each response to the overall list
  ret_list = []
  for term_id, weight in big_list: #Appends only the Bigram to ret_list
    ret_list.append(dictionary.get(term_id))
  return ret_list

"""#Create Similarity key/value Pairs"""

def create_similarity(lst_words):
  nlp = spacy.load('en_core_web_md')
  cat = {}
  for word in lst_words: 
    sim = {}
    for comp in lst_words:
      wd = nlp(word)
      cp = nlp(comp)
      sim[comp] = wd.similarity(cp) #Calculates similarity between two Bigrams
    sim = sorted(sim.items(), key=lambda kv: kv[1],reverse=True) #Sort similarities based on calculated similarity
    if sim[0][1] == 1.0: #Checks if its the same Bigram
      cat[word]=sim[1][0] #If it is, append next highest ranking Bigram
    else:
      cat[word]=sim[0][0] #Appends highest ranking Bigram
  return cat

"""#Creates First Round of Groupings"""

def recursive(key,dictionary, group_list):
  group_list.append(key)
  v = dictionary[key]
  if v not in group_list:
    recursive(v,dictionary,group_list)
  return group_list

def get_sets(word_assoc):
  lol = []
  for key in list(word_assoc.keys()):
      emp = [] #list for each groups
      emp = set(recursive(key,word_assoc,[]))
      lol.append(emp)
  return lol

"""#Create Final Groups"""

def create_groups(list_of_sets):
  i = len(list_of_sets) - 1
  while i > 0: #Loops from the back to the front
    j = i-1 #Checks the bigram before the current one
    while j >= 0:
      if not list_of_sets[i].isdisjoint(list_of_sets[j]): #Checks for at least one similar Bigram
        list_of_sets[i].update(list_of_sets[j]) #Merges the two sets into one set
        del list_of_sets[j] #Deletes the extra set
        i-= 1
      j-=1
    i-=1
  return list_of_sets #returns list of finalized sets

"""#Assistance from Faculty

https://docs.python.org/3/library/stdtypes.html#set-types-set-frozenset

Here's some pseudo-code suggestions based on your email and what you have:

Turn each list from get_groups(word_assoc) into a "set" type. (may want frozen set type ) (may need to turn sublists into set/frozenset first).

bgram_sets = frozenset(get_groups(word_assoc))

for x in bgram_sets:

  x.isdisjoint( bgrams_sets(excluding x) ) <-- gives true/false for set overlaps.

  Then do something with the false-values to merge sets etc. 

  --perhaps this could be done in a recursively called function instead that also merges the two sets if they aren't disjoint --

#Run all the functions to create final groups
"""

stud_topics_final_groups = create_groups(list(get_sets(create_similarity(senttfidf(stud_topics_ngrams)))))
print('Student Topic Groups:')
for group in stud_topics_final_groups:
  print(group)
print('\n')
profta_topics_final_groups = create_groups(list(get_sets(create_similarity(senttfidf(profta_topics_ngrams)))))
print('Professor/TA Topic Groups:')
for group in profta_topics_final_groups:
  print(group)

print('\n')
print('\n')

stud_struggles_final_groups = create_groups(list(get_sets(create_similarity(senttfidf(stud_struggles_ngrams)))))
print('Student Struggles Groups:')
for group in stud_struggles_final_groups:
  print(group)
print('\n')
profta_struggles_final_groups = create_groups(list(get_sets(create_similarity(senttfidf(profta_struggles_ngrams)))))
print('Professor/TA Struggles Groups:')
for group in profta_struggles_final_groups:
  print(group)

print('\n')
print('\n')

stud_questions_final_groups = create_groups(list(get_sets(create_similarity(senttfidf(stud_questions_ngrams)))))
print('Student Questions Groups:')
for group in stud_questions_final_groups:
  print(group)
print('\n')
profta_questions_final_groups = create_groups(list(get_sets(create_similarity(senttfidf(profta_questions_ngrams)))))
print('Professor/TA Questions Groups:')
for group in profta_questions_final_groups:
  print(group)

print('\n')
print('\n')

stud_surprise_final_groups = create_groups(list(get_sets(create_similarity(senttfidf(stud_surprise_ngrams)))))
print('Student Surprise Questions Groups:')
for group in stud_surprise_final_groups:
  print(group)
print('\n')
profta_surprise_final_groups = create_groups(list(get_sets(create_similarity(senttfidf(profta_surprise_ngrams)))))
print('Professor/TA Surprise Questions Groups:')
for group in profta_surprise_final_groups:
  print(group)
